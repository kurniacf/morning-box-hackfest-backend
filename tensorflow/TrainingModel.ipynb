{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model AI Generated Menu Morning Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "\n",
    "# Pandas and Numpy are used for data manipulation and preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# OneHotEncoder and MultiLabelBinarizer for encoding categorical variables and splitting the data into training and testing sets\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TensorFlow with Keras for building the neural network model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "data = pd.read_csv('Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert survey answers to numeric\n",
    "data[['survey_1', 'survey_2', 'survey_3', 'survey_4', 'survey_5']] = data[[\n",
    "    'survey_1','survey_2', 'survey_3', 'survey_4', 'survey_5']\n",
    "    ].applymap(lambda x: 0 if x == 'A' else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for allergy column\n",
    "# The code below is for preprocessing the allergy column of the dataset\n",
    "\n",
    "# Convert non-string values to empty strings and remove the square brackets and double quotes\n",
    "data['survey_6'] = data['survey_6'].apply(lambda x: x.strip(\n",
    "    '[]').replace('\"', '').split(',') if isinstance(x, str) else '')\n",
    "\n",
    "# Initialize the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Apply one-hot encoding to the allergy column using MultiLabelBinarizer\n",
    "encoded_allergies = mlb.fit_transform(data['survey_6'])\n",
    "\n",
    "# Create column names\n",
    "allergy_columns = ['allergy_' + col for col in mlb.classes_]\n",
    "\n",
    "# Convert the encoded allergies array to a DataFrame and concat\n",
    "encoded_allergies_df = pd.DataFrame(encoded_allergies, columns=allergy_columns)\n",
    "data = pd.concat([data, encoded_allergies_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target labels\n",
    "target_columns = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "y = data[target_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels\n",
    "encoder = OneHotEncoder()\n",
    "y_encoded = encoder.fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = data[['survey_1', 'survey_2', 'survey_3', 'survey_4', 'survey_5'] + allergy_columns].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the model\n",
    "# Create a sequential model, which is a linear stack of layers\n",
    "model = tf.keras.Sequential([\n",
    "    # Add a dense layer with 64 neurons, ReLU activation function, and input shape matching the number of features in X_train\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    # Add another dense layer with 64 neurons and ReLU activation function\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    # Add the output layer with neurons equal to the number of categories in y_encoded and a softmax activation function\n",
    "    layers.Dense(y_encoded.shape[1], activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 591ms/step - loss: 13.7042 - accuracy: 0.0000e+00 - val_loss: 13.6999 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 13.6999 - accuracy: 0.0000e+00 - val_loss: 13.6959 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.6959 - accuracy: 0.0000e+00 - val_loss: 13.6930 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.6930 - accuracy: 0.0000e+00 - val_loss: 13.6904 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.6904 - accuracy: 0.0000e+00 - val_loss: 13.6881 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 13.6881 - accuracy: 0.0000e+00 - val_loss: 13.6864 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 13.6864 - accuracy: 0.0000e+00 - val_loss: 13.6851 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.6851 - accuracy: 0.0000e+00 - val_loss: 13.6840 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 13.6840 - accuracy: 0.0000e+00 - val_loss: 13.6837 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.6837 - accuracy: 0.0000e+00 - val_loss: 13.6839 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.6839 - accuracy: 0.0000e+00 - val_loss: 13.6846 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.6846 - accuracy: 0.0000e+00 - val_loss: 13.6857 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.6857 - accuracy: 0.0000e+00 - val_loss: 13.6871 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 13.6871 - accuracy: 0.0000e+00 - val_loss: 13.6885 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.6885 - accuracy: 0.0000e+00 - val_loss: 13.6902 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.6902 - accuracy: 0.0000e+00 - val_loss: 13.6921 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 13.6921 - accuracy: 0.0000e+00 - val_loss: 13.6940 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.6940 - accuracy: 0.0000e+00 - val_loss: 13.6959 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 13.6959 - accuracy: 0.0000e+00 - val_loss: 13.6981 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 13.6981 - accuracy: 0.0000e+00 - val_loss: 13.7004 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Configure the model for training by specifying the optimizer, loss function, and metric to be used\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the training data (X_train, y_train), validating it against the test data (X_test, y_test)\n",
    "# The training process will run for 20 epochs, with a batch size of 32 samples\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "sample_input = np.array([X_test[0]])  # Replace with your input data\n",
    "predictions = model.predict(sample_input)\n",
    "recommendations = encoder.inverse_transform(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'monday': nan, 'tuesday': nan, 'wednesday': nan, 'thursday': nan, 'friday': nan, 'saturday': nan, 'sunday': nan}\n"
     ]
    }
   ],
   "source": [
    "# Print recommendations\n",
    "day_names = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "recommended_meals = {day: meal for day, meal in zip(day_names, recommendations[0])}\n",
    "print(recommended_meals)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
