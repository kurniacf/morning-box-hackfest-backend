{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model AI Generated Menu Morning Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "\n",
    "# Pandas and Numpy are used for data manipulation and preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# OneHotEncoder and MultiLabelBinarizer for encoding categorical variables and splitting the data into training and testing sets\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TensorFlow with Keras for building the neural network model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "data = pd.read_csv('Dataset.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id survey_1 survey_2 survey_3 survey_4 survey_5  \\\n",
      "0   1        A        B        A        A        B   \n",
      "1   2        B        A        B        B        A   \n",
      "2   3        A        B        B        A        B   \n",
      "3   4        B        A        A        B        A   \n",
      "4   5        A        B        A        B        B   \n",
      "\n",
      "                     survey_6                monday               tuesday  \\\n",
      "0    Telur, Kacang - kacangan  3tqsB3n37DmaFcFq2v6X  82PPspsOvyVK034pq52a   \n",
      "1            Seafood, Kedelai  QqfyjPHylFOFrx82jA2O  T6aI1OQQaFx8BzDDtnE6   \n",
      "2               Ikan, Seafood  y4RgqqYAHBCnGxBMkZTS  y6MzIGk0oHsAEDJRCbBE   \n",
      "3              Telur, Kedelai  Gd3js09RMKe02enHWPya  LOK0YSMFj5c5ov2IvxM2   \n",
      "4  Kacang - kacangan, Seafood  uC3UbmTGj3ymYIjd7PCa  uKPyjZSkUy1y349n9uhu   \n",
      "\n",
      "              wednesday              thursday                friday  \\\n",
      "0  B96GahULQbBIPVBC3LHj  FL7IqcGuEfgZcAGnMAqW  Gd3js09RMKe02enHWPya   \n",
      "1  WtCirfXQ0ZK99opIGTaG  uC3UbmTGj3ymYIjd7PCa  uKPyjZSkUy1y349n9uhu   \n",
      "2  zMAK1zOqQH3ayYtVfjTZ  3tqsB3n37DmaFcFq2v6X  82PPspsOvyVK034pq52a   \n",
      "3  OGUCWGn7nTq7mmJn7NTQ  QqfyjPHylFOFrx82jA2O  T6aI1OQQaFx8BzDDtnE6   \n",
      "4  xlCldGyo9qpLZeJkvpFL  y4RgqqYAHBCnGxBMkZTS  y6MzIGk0oHsAEDJRCbBE   \n",
      "\n",
      "               saturday                sunday  \n",
      "0  LOK0YSMFj5c5ov2IvxM2  OGUCWGn7nTq7mmJn7NTQ  \n",
      "1  xlCldGyo9qpLZeJkvpFL                   NaN  \n",
      "2  B96GahULQbBIPVBC3LHj  FL7IqcGuEfgZcAGnMAqW  \n",
      "3  WtCirfXQ0ZK99opIGTaG                   NaN  \n",
      "4  zMAK1zOqQH3ayYtVfjTZ                   NaN  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['survey_1', 'survey_2', 'survey_3', 'survey_4', 'survey_5']\n",
    "\n",
    "for column in column_names:\n",
    "    if column in data.columns:\n",
    "        data[column] = data[column].apply(lambda x: 0 if x == 'A' else 1)\n",
    "    else:\n",
    "        print(f\"Column '{column}' not found in DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert survey answers to numeric\n",
    "data[['survey_1', 'survey_2', 'survey_3', 'survey_4', 'survey_5']] = data[[\n",
    "    'survey_1','survey_2', 'survey_3', 'survey_4', 'survey_5']\n",
    "    ].applymap(lambda x: 0 if x == 'A' else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for allergy column\n",
    "# The code below is for preprocessing the allergy column of the dataset\n",
    "\n",
    "# Convert non-string values to empty strings and remove the square brackets and double quotes\n",
    "data['survey_6'] = data['survey_6'].apply(lambda x: x.strip(\n",
    "    '[]').replace('\"', '').split(',') if isinstance(x, str) else '')\n",
    "\n",
    "# Initialize the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Apply one-hot encoding to the allergy column using MultiLabelBinarizer\n",
    "encoded_allergies = mlb.fit_transform(data['survey_6'])\n",
    "\n",
    "# Create column names\n",
    "allergy_columns = ['allergy_' + col for col in mlb.classes_]\n",
    "\n",
    "# Convert the encoded allergies array to a DataFrame and concat\n",
    "encoded_allergies_df = pd.DataFrame(encoded_allergies, columns=allergy_columns)\n",
    "data = pd.concat([data, encoded_allergies_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target labels\n",
    "target_columns = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "y = data[target_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels\n",
    "encoder = OneHotEncoder()\n",
    "y_encoded = encoder.fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = data[['survey_1', 'survey_2', 'survey_3', 'survey_4', 'survey_5'] + allergy_columns].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the model\n",
    "# Create a sequential model, which is a linear stack of layers\n",
    "model = tf.keras.Sequential([\n",
    "    # Add a dense layer with 64 neurons, ReLU activation function, and input shape matching the number of features in X_train\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    # Add another dense layer with 64 neurons and ReLU activation function\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    # Add the output layer with neurons equal to the number of categories in y_encoded and a softmax activation function\n",
    "    layers.Dense(y_encoded.shape[1], activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 29.0029 - accuracy: 0.0000e+00 - val_loss: 29.4421 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 28.9591 - accuracy: 0.1250 - val_loss: 29.5909 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 28.9168 - accuracy: 0.1250 - val_loss: 29.7350 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 28.8785 - accuracy: 0.1250 - val_loss: 29.8722 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 28.8441 - accuracy: 0.1250 - val_loss: 30.0040 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 28.8150 - accuracy: 0.1250 - val_loss: 30.1369 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 28.7873 - accuracy: 0.1250 - val_loss: 30.2753 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 28.7607 - accuracy: 0.1250 - val_loss: 30.4206 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 28.7349 - accuracy: 0.0000e+00 - val_loss: 30.5726 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 28.7102 - accuracy: 0.0000e+00 - val_loss: 30.7331 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 28.6860 - accuracy: 0.0000e+00 - val_loss: 30.9031 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 28.6623 - accuracy: 0.0000e+00 - val_loss: 31.0851 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 28.6401 - accuracy: 0.0000e+00 - val_loss: 31.2801 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 28.6181 - accuracy: 0.0000e+00 - val_loss: 31.4922 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 28.5959 - accuracy: 0.0000e+00 - val_loss: 31.7217 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 28.5735 - accuracy: 0.0000e+00 - val_loss: 31.9671 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 28.5510 - accuracy: 0.0000e+00 - val_loss: 32.2304 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 28.5289 - accuracy: 0.0000e+00 - val_loss: 32.5127 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 28.5064 - accuracy: 0.0000e+00 - val_loss: 32.8145 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 28.4846 - accuracy: 0.0000e+00 - val_loss: 33.1357 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Configure the model for training by specifying the optimizer, loss function, and metric to be used\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the training data (X_train, y_train), validating it against the test data (X_test, y_test)\n",
    "# The training process will run for 20 epochs, with a batch size of 32 samples\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "sample_input = np.array([X_test[0]])  # Replace with your input data\n",
    "predictions = model.predict(sample_input)\n",
    "recommendations = encoder.inverse_transform(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'monday': 'FL7IqcGuEfgZcAGnMAqW', 'tuesday': 'T6aI1OQQaFx8BzDDtnE6', 'wednesday': 'zMAK1zOqQH3ayYtVfjTZ', 'thursday': 'uC3UbmTGj3ymYIjd7PCa', 'friday': 'T6aI1OQQaFx8BzDDtnE6', 'saturday': 'xlCldGyo9qpLZeJkvpFL', 'sunday': 'WtCirfXQ0ZK99opIGTaG'}\n"
     ]
    }
   ],
   "source": [
    "# Print recommendations\n",
    "day_names = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "recommended_meals = {day: meal for day, meal in zip(day_names, recommendations[0])}\n",
    "print(recommended_meals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
